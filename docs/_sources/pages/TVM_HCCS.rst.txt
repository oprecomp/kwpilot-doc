..  Licensed to the Apache Software Foundation (ASF) under one
    or more contributor license agreements.  See the NOTICE file
    distributed with this work for additional information
    regarding copyright ownership.  The ASF licenses this file
    to you under the Apache License, Version 2.0 (the
    "License"); you may not use this file except in compliance
    with the License.  You may obtain a copy of the License at

..    http://www.apache.org/licenses/LICENSE-2.0

..  Unless required by applicable law or agreed to in writing,
    software distributed under the License is distributed on an
    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    KIND, either express or implied.  See the License for the
    specific language governing permissions and limitations
    under the License.

.. _installation:

TVM setup on HCCS
==================
We have setup TVM on a single PC node on K211 with 2 PYNQ boards with VTA overlays.

+---------------------+-------------------------+------+------------------------------------------------+
| Node                | Device                  | TVM  | Pynq image                                     |
+---------------------+-------------------------+------+------------------------------------------------+
| poltera  9.4.68.178 | PYNQ1/2 192.168.2.98/99 | v0.5 | v2.4 (https://github.com/Xilinx/PYNQ/releases) |
+---------------------+-------------------------+------+------------------------------------------------+


Environment on Host
------------------

Consider to add the following lines to your `.bashrc`:

.. code:: bash

          export PYTHONPATH=$TVM_HOME/python:$TVM_HOME/topi/python:$TVM_HOME/nnvm/python:$TVM_HOME/vta/python:${PYTHONPATH}
          export PYTHONPATH=$TVM_HOME/vta/python:${PYTHONPATH}


          export XILINX_ROOT=/tools/Xilinx
          export XILINX_VERSION=2018.3
          source $XILINX_ROOT/Vivado/$XILINX_VERSION/.settings64-Vivado.sh

          export VTA_PYNQ_RPC_HOST=192.168.2.98
          export VTA_PYNQ_RPC_PORT=9091
          export TVM_TRACKER_HOST=192.168.2.1
          export TVM_TRACKER_PORT=9190

.. note:: Don't forget ``$TVM_HOME/vta/pyton`` since it is not mentioned in the usual TVM tutorial.

To access the PYNQ boards with ssh-keys, download the private keys from `here <https://ibm.box.com/s/3l4gw8gp3fu7k0klzfe8mlgr8qc3uc8i>`_ and copy it to your home on ``poltera``.
Additionally, add this to ``~/.ssh/config``:

.. code:: bash


          Host pynq1
            HostName 192.168.2.98
            User xilinx
            IdentityFile ~/.ssh/id_draico_dev

          Host pynq2
            HostName 192.168.2.99
            User xilinx
            IdentityFile ~/.ssh/id_draico_dev


Setup of HCCS_TVM development environment
--------------------------------------------

In order to do our own development on TVM (and publish later), we created our own fork: https://github.ibm.com/cloudFPGA/hccs_tvm

1. Follow the setup of ``virutalenv`` in the Readme `here <https://github.ibm.com/cloudFPGA/hccs_tvm/blob/master/README.md>`_.

2. Execute [this]_ and [this]_ setup with your local clone.
   In summary:

.. code:: bash


      mkdir build
      cp cmake/config.cmake build/
      which llvm-config-9 #copy result!
      vim build/config.cmake
      #-> change USE LLVM to (around line 104) and UNCOMMENT it
        set(USE_LLVM /usr/bin/llvm-config-9)
      cd build
      cmake ..
      make -j4 # or how many cores you want to use
      # will take some time

.. [this] https://docs.tvm.ai/install/from_source.html#build-the-shared-library
.. [this] https://docs.tvm.ai/install/from_source.html#python-package-installation

3. Done, now run the test below


VTA Test
=============

In a minimum setup we will need to open 4 terminals in the following order 1->2->3->4 (1,2,4 on host, 3 on PYNqs):

.. code:: bash

            Host                                    Device
            ------------------------------          ------------------------------
            |                            |   rpc    |                            |
            | 1. Start RPC Tracker       |   <-->   | 3. Start RPC Server        |
            | 2. Start RPC Tracker Query |          |                            |
            | 4. Run tutorial            |          |                            |
            ------------------------------          ------------------------------


On Host
-------

.. note:: Make sure ``TVM_HOME`` and ``PYTHONPATH`` are set correctly in your ``~/.bashrc``.


1. Start RPC tracker:
  - ``ssh poltera``
  - ``cd your/hccs_tvm/clone``
  - ``source py_tvm/bin/activate``
  - ``python3 -m tvm.exec.rpc_tracker --host=0.0.0.0 --port=9190``

2. Query the tracker:
  - ``ssh poltera``
  - ``cd your/hccs_tvm/clone``
  - ``source py_tvm/bin/activate``
  - ``watch python3 -m tvm.exec.query_rpc_tracker --host=0.0.0.0 --port=9190``

4. Run examplary code tutorial
  - ``ssh poltera``
  - ``cd your/hccs_tvm/clone``
  - ``source py_tvm/bin/activate``
  - A 2d convolution
      - ``python3 .vta/tests/python/integration/test_benchmark_topi_conv2d.py``
  - Autotuning
      - ``cd ./tutorials/autotune``
      - Tuning for Nano CPU: ``python3 tune_relay_arm.py``
      - Tuning for PYNQ VTA: ``python3 tune_relay_vta.py``


On PYNQ Devices
---------------

.. note:: Considering tvm runtime installation at **/home/xilinx/tvm** for PYNQs and **/home/did/tvm** for Jetson

3. Start RPC server:
  - Connect to poltera
      - ``ssh poltera``
  - Connect to devices through poltera
      - PYNQ 1: ``ssh xilinx@192.168.2.98`` (or, with the ssh config above: ``ssh pynq1``)
      - PYNQ 2: ``ssh xilinx@192.168.2.99`` (``ssh pynq2``)
      - Jetson Nano: ``ssh didx@192.168.2.100``
  - Start RPC server on device
      - PYNQ: ``sudo /home/xilinx/tvm/apps/vta_rpc/start_rpc_server_to_tracker.py``  (Change ``fleet`` to tracker URL)
      - Nano: ```python3 -m tvm.exec.rpc_server --tracker 192.168.2.1:9190 --key=nano``


Visualization
=============

We are using `netron <https://github.com/lutzroeder/netron>`_ to visualize neural network models.
  - ``wget https://s3.amazonaws.com/onnx-model-zoo/resnet/resnet18v1/resnet18v1.onnx``
  - ``netron ./resnet18v1.onnx``
